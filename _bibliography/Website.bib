
@article{lidard_feedback_2021,
	title = {Feedback {Control} and {Parameter} {Estimation} for {Lift} {Maximization} of a {Pitching} {Airfoil}},
	volume = {44},
	issn = {1533-3884},
	url = {https://arc.aiaa.org/doi/10.2514/1.G005441},
	doi = {10.2514/1.G005441},
	language = {en},
	number = {3},
	urldate = {2022-04-20},
	journal = {Journal of Guidance, Control, and Dynamics},
	author = {Lidard, Justin M. and Goswami, Debdipta and Snyder, David and Sedky, Girguis and Jones, Anya R. and Paley, Derek A.},
	month = mar,
	year = {2021},
	pages = {587--594},
	file = {Lidard et al. - 2021 - Feedback Control and Parameter Estimation for Lift.pdf:/Users/justinlidard/Zotero/storage/PCNCR8YE/Lidard et al. - 2021 - Feedback Control and Parameter Estimation for Lift.pdf:application/pdf},
}

@article{lidard_provably_2022,
	title = {Provably {Efficient} {Multi}-{Agent} {Reinforcement} {Learning} with {Fully} {Decentralized} {Communication}},
	url = {http://arxiv.org/abs/2110.07392},
	abstract = {A challenge in reinforcement learning (RL) is minimizing the cost of sampling associated with exploration. Distributed exploration reduces sampling complexity in multi-agent RL (MARL). We investigate the benefits to performance in MARL when exploration is fully decentralized. Specifically, we consider a class of online, episodic, tabular \$Q\$-learning problems under time-varying reward and transition dynamics, in which agents can communicate in a decentralized manner.We show that group performance, as measured by the bound on regret, can be significantly improved through communication when each agent uses a decentralized message-passing protocol, even when limited to sending information up to its \${\textbackslash}gamma\$-hop neighbors. We prove regret and sample complexity bounds that depend on the number of agents, communication network structure and \${\textbackslash}gamma.\$ We show that incorporating more agents and more information sharing into the group learning scheme speeds up convergence to the optimal policy. Numerical simulations illustrate our results and validate our theoretical claims.},
	urldate = {2022-04-20},
	journal = {American Control Conference (Accepted)},
	author = {Lidard, Justin and Madhushani, Udari and Leonard, Naomi Ehrich},
	month = jun,
	year = {2022},
	note = {arXiv: 2110.07392},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/justinlidard/Zotero/storage/X98JHTPR/Lidard et al. - 2021 - Provably Efficient Multi-Agent Reinforcement Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/justinlidard/Zotero/storage/VWR2Y43J/2110.html:text/html},
}
